{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Prepare a function named binary_relevance_ldc which will implement the Binary Relevance\n",
    "Method for multilabel classification. The classifier model for the binary classification should be the\n",
    "Linear Discriminant Classifier (LDC). The input should be: training data (N × n), training labels\n",
    "(N × c, containing integers 1, 2, 3, ...), and testing data (Ntest × n). The function should return a\n",
    "binary matrix of assigned labels of size Ntest × c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "def binary_relevance_ldc(train_data, train_binary_labels, test_data):\n",
    "    N, n = train_data.shape\n",
    "    N_test, _ = test_data.shape\n",
    "    c = train_binary_labels.shape[1]\n",
    "\n",
    "    ldc_classifiers = []\n",
    "    for j in range(c):\n",
    "        ldc = LinearDiscriminantAnalysis()\n",
    "        ldc.fit(train_data, train_binary_labels[:, j])\n",
    "        ldc_classifiers.append(ldc)\n",
    "\n",
    "    assigned_labels = np.zeros((N_test, c))\n",
    "    for i, ldc_classifier in enumerate(ldc_classifiers):\n",
    "        predictions = ldc_classifier.predict(test_data)\n",
    "        assigned_labels[:, i] = predictions\n",
    "\n",
    "    return assigned_labels\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Subsequently, write a function named adaptive_knn which will implement the Adaptive knn\n",
    "methods for multilabel classification as explained in the lectures (The slide is reprodiced in Figure 1)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's the revised solution for the adaptive_knn function using only the specified libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "def adaptive_knn(train_data, train_labels, test_data, k=10, threshold=3):\n",
    "    N, n = train_data.shape\n",
    "    N_test, _ = test_data.shape\n",
    "    c = train_labels.max()\n",
    "\n",
    "    binary_labels = np.zeros((N, c))\n",
    "    for i in range(N):\n",
    "        binary_labels[i, train_labels[i] - 1] = 1\n",
    "\n",
    "    assigned_labels = np.zeros((N_test, c))\n",
    "\n",
    "    for i, x in enumerate(test_data):\n",
    "        distances = np.array([euclidean_distance(x, train_data[j]) for j in range(N)])\n",
    "        sorted_indices = np.argsort(distances)[:k]\n",
    "        neighbors_labels = binary_labels[sorted_indices, :]\n",
    "        membership_counting_vector = np.sum(neighbors_labels, axis=0)\n",
    "\n",
    "        for j in range(c):\n",
    "            if membership_counting_vector[j] > threshold:\n",
    "                assigned_labels[i, j] = 1\n",
    "\n",
    "    return assigned_labels\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Use the Bird data set from Lab 6. (The csv files are also provided with this script for convenience.)\n",
    "Apply the two functions you programmed in (a) and (b) to compare the two classifiers for multilabel\n",
    "data on this dataset. Program and use the hamming loss for your comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss for Binary Relevance LDC: 0.1952093856933355\n",
      "Hamming Loss for Adaptive kNN: 0.948997881701157\n"
     ]
    }
   ],
   "source": [
    "def hamming_loss(y_true, y_pred):\n",
    "    return np.mean(np.not_equal(y_true, y_pred))\n",
    "\n",
    "def bird_accuracies(y_true, y_pred):\n",
    "    bird_accuracies = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        accuracy = np.sum((y_true[:, i] == y_pred[:, i])) / y_true.shape[0]\n",
    "        bird_accuracies.append(accuracy)\n",
    "    return np.array(bird_accuracies)\n",
    "\n",
    "# Load the train and test datasets:\n",
    "train_data = pd.read_csv('csv_result-birds-train.csv')\n",
    "test_data = pd.read_csv('csv_result-birds-test.csv')\n",
    "\n",
    "# Separate the features and labels in both train and test datasets:\n",
    "train_features = train_data.iloc[:, :-19].values\n",
    "train_labels = train_data.iloc[:, -19:].values\n",
    "test_features = test_data.iloc[:, :-19].values\n",
    "test_labels = test_data.iloc[:, -19:].values\n",
    "\n",
    "# Apply the two classifiers on the dataset\n",
    "assigned_labels_ldc = binary_relevance_ldc(train_features, train_labels, test_features)\n",
    "assigned_labels_knn = adaptive_knn(train_features, train_labels, test_features)\n",
    "\n",
    "# Calculate the Hamming loss for each classifier\n",
    "hamming_loss_ldc = hamming_loss(test_labels, assigned_labels_ldc)\n",
    "hamming_loss_knn = hamming_loss(test_labels, assigned_labels_knn)\n",
    "\n",
    "# Print the Hamming loss for each classifier\n",
    "print(\"Hamming Loss for Binary Relevance LDC:\", hamming_loss_ldc)\n",
    "print(\"Hamming Loss for Adaptive kNN:\", hamming_loss_knn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Find out the bird that is most well recognised and the one that is least well recognised. Find\n",
    "images of these birds and show them in a figure. You may use plt.imread() and plt.imshow(). An example of the expected output is shown in Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most well-recognized bird index: 13\n",
      "Least well-recognized bird index: 7\n"
     ]
    }
   ],
   "source": [
    "# Calculate bird accuracies\n",
    "bird_accuracies_ldc = bird_accuracies(test_labels, assigned_labels_ldc)\n",
    "\n",
    "# Find the indices for the most and least well-recognized birds\n",
    "max_index = np.argmax(bird_accuracies_ldc)\n",
    "min_index = np.argmin(bird_accuracies_ldc)\n",
    "\n",
    "print(\"Most well-recognized bird index:\", max_index)\n",
    "print(\"Least well-recognized bird index:\", min_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2. Streaming data\n",
    "Consider the following experiment. A data stream comes from two classes with normal distributions.\n",
    "The distributions are static; their parameters don’t change with time. The parameters are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "486e0d5a79acdbfffd563ee7a67a93a5017bd2a4f66495483a69f0245c8a4a6c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
